{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import module.crawler as cr\n",
    "import module.costum_parser as pr\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.settings import Settings\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings the necessary direcory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "pages_path = os.path.join('data', 'pages')\n",
    "os.makedirs(pages_path, exist_ok=True)\n",
    "\n",
    "data_path = os.path.join('data', 'data_tsv')\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "\n",
    "dataset_path = os.path.join('data', 'dataset.tsv')\n",
    "urls_path = os.path.join('data', 'urls.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get the list of Michelin restaurants\n",
    "\n",
    "You should begin by compiling a list of restaurants to include in your document corpus. Specifically, you will focus on web scraping the [Michelin Restaurants in Italy](https://guide.michelin.com/en/it/restaurants). Your task is to **collect the URL** associated with each restaurant in this list. The output of this step should be a `.txt` file where each line contains a single restaurant’s URL. By the end, you should have approximately 2,037 restaurants on your list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom settings for the url spider\n",
    "custom_settings = Settings({\n",
    "    'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',  # Set to recommended value to avoid issues\n",
    "    'LOG_LEVEL': 'ERROR'  # Suppress other logging\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_process = CrawlerProcess(settings=custom_settings) # Create a process for the spider\n",
    "get_url_process.crawl(cr.UrlMichelin, urls_path) # Add the spider to the process\n",
    "get_url_process.start() # Run the spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in file: 1983\n"
     ]
    }
   ],
   "source": [
    "# Check if the file exists\n",
    "if os.path.exists(urls_path):\n",
    "    # Check the output file and see if the number of lines is correct\n",
    "    lines_in_file = open(urls_path, 'r').readlines()\n",
    "    number_of_lines = len(lines_in_file)\n",
    "    print(f'Number of lines in file: {number_of_lines}')\n",
    "else:\n",
    "    print('Failure: File not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl Michelin restaurant pages\n",
    "\n",
    "Once you have all the URLs on the list, you should:\n",
    "\n",
    "1. Download the HTML corresponding to each of the collected URLs.\n",
    "2. After collecting each page, immediately save its `HTML` in a file. This way, if your program stops for any reason, you will not lose the data collected up to the stopping point.\n",
    "3. Organize the downloaded `HTML` pages into folders. Each folder will contain the `HTML` of the restaurants from page 1, page 2, ... of the Michelin restaurant list.\n",
    "\n",
    "__Tip__: Due to the large number of pages to download, consider using methods that can help shorten the process. If you employed a particular process or approach, kindly describe it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded all pages!\n"
     ]
    }
   ],
   "source": [
    "lines_of_urls = []\n",
    "with open(urls_path, 'r') as file:\n",
    "    lines_of_urls = file.readlines()\n",
    "\n",
    "original_directory = os.getcwd()\n",
    "os.chdir(os.path.join(original_directory, pages_path))\n",
    "\n",
    "# Create folders for the HTML files\n",
    "cr.make_folders(100)\n",
    "\n",
    "max_w = os.cpu_count()\n",
    "\n",
    "# Download the HTML files concurrently\n",
    "with ThreadPoolExecutor(max_workers=max_w) as executor:\n",
    "    download_futures = []\n",
    "    for line in lines_of_urls:\n",
    "        # Split the line into URL and page number\n",
    "        page_num = int(line.split(\"|\")[1])\n",
    "        url = line.split(\"|\")[0].strip()\n",
    "                \n",
    "        # Submit download task to the executor\n",
    "        download_futures.append(executor.submit(cr.HTML_downloader, url, page_num))\n",
    "\n",
    "    # Wait for all tasks to complete\n",
    "    for future in as_completed(download_futures):\n",
    "        try:\n",
    "            future.result()\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Notify completion\n",
    "print(\"Downloaded all pages!\")\n",
    "\n",
    "# Return to the original directory\n",
    "os.chdir(original_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File count: 1983\n"
     ]
    }
   ],
   "source": [
    "# Check if the files exist and are 1983\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "dir_path = os.path.join(current_dir, pages_path)\n",
    "os.chdir(dir_path)\n",
    "count = 0\n",
    "\n",
    "for i in range(1,101):\n",
    "    folder = f'page_{i}'\n",
    "    for path in os.listdir(folder):\n",
    "        if os.path.isfile(os.path.join(folder, path)):\n",
    "            count += 1\n",
    "\n",
    "os.chdir(current_dir)\n",
    "\n",
    "print('File count:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n",
    "\n",
    "At this point, you should have all the HTML documents about the restaurant of interest, and you can start to extract specific information. The list of the information we desire for each restaurant and their format is as follows:\n",
    "\n",
    "1. **Restaurant Name** (to save as `restaurantName`): string;\n",
    "2. **Address** (to save as `address`): string;\n",
    "3. **City** (to save as `city`): string;\n",
    "4. **Postal Code** (to save as `postalCode`): string;\n",
    "5. **Country** (to save as `country`): string;\n",
    "6. **Price Range** (to save as `priceRange`): string;\n",
    "7. **Cuisine Type** (to save as `cuisineType`): string;\n",
    "8. **Description** (to save as `description`): string;\n",
    "9. **Facilities and Services** (to save as `facilitiesServices`): list of strings;\n",
    "10. **Accepted Credit Cards** (to save as `creditCards`): list of strings;\n",
    "11. **Phone Number** (to save as `phoneNumber`): string;\n",
    "12. **URL to the Restaurant Page** (to save as `website`): string.\n",
    "\n",
    "For each restaurant, you create a `restaurant_i.tsv` file of this structure:\n",
    "\n",
    "```\n",
    "restaurantName \\t address \\t  ... \\t url\n",
    "```\n",
    "\n",
    "If an information is missing, you just leave it as an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted all data!\n"
     ]
    }
   ],
   "source": [
    "keys = ['index', 'restaurantName', 'address', 'city', 'postalCode', 'country', 'priceRange', 'cuisineType', 'description', 'creditCards', 'facilitiesServices', 'phoneNumber', 'website']\n",
    "\n",
    "max_w = os.cpu_count()\n",
    "\n",
    "# Download the data from HTML files concurrently\n",
    "with ThreadPoolExecutor(max_workers=max_w) as executor:\n",
    "    extractor_future = []\n",
    "    for i in range(1,101):\n",
    "        start_dir = os.path.join(pages_path, f'page_{i}')\n",
    "        start_index = (i-1)*20 \n",
    "        extractor_future.append(executor.submit(pr.tsv_extractor, start_dir, data_path, start_index, keys))\n",
    "\n",
    "    # Wait for all tasks to complete\n",
    "    for future in as_completed(extractor_future):\n",
    "        try:\n",
    "            future.result()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "# Notify completion\n",
    "print(\"Extracted all data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File count: 1983\n"
     ]
    }
   ],
   "source": [
    "# Check if the files exist and are 1983\n",
    "count = 0\n",
    "\n",
    "for path in os.listdir(data_path):\n",
    "        if os.path.isfile(os.path.join(data_path, path)):\n",
    "            count += 1\n",
    "\n",
    "print('File count:', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified all data!\n"
     ]
    }
   ],
   "source": [
    "# Unify all data into one file csv\n",
    "\n",
    "# List all TSV files in the directory\n",
    "tsv_files = [f for f in os.listdir(data_path) if f.endswith('.tsv')]\n",
    "\n",
    "# Load all TSV files into a list of dataframes\n",
    "dfs = [pd.read_csv(os.path.join(data_path, file), sep='\\t') for file in tsv_files]\n",
    "\n",
    "# Unite all dataframes into one\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "merged_df.sort_values(by=['index'], inplace=True)\n",
    "\n",
    "# Save the merged dataframe to a TSV file\n",
    "merged_df.to_csv(dataset_path, sep='\\t', index=False)\n",
    "\n",
    "# Notify completion\n",
    "print(\"Unified all data!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Search Engine\n",
    "\n",
    "This search engine allows you to retrieve restaurants based on a user query. We’ll build two types of search engines:\n",
    "\n",
    "- Conjunctive Search Engine: Returns restaurants where all query terms appear in the description.\n",
    "- Ranked Search Engine: Returns the top-k restaurants sorted by similarity to the query, using TF-IDF and Cosine Similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To effectively analyze restaurant descriptions, it is crucial to *pre-process the text*. As in any optimal text analysis, we must proceed with preprocessing, which we addressed in the first part.In general we followed these steps.:\n",
    "\n",
    "- Firstly we ensured the removal of stop words in English, as well as customized common words related to Italian cuisine and gourmet dining, such as \"pasta,\" \"pizza,\" and other frequently used terms. \n",
    "\n",
    "- The next step involved constructing a `vocabulary` to extract all unique words from the various descriptions and associate each with a unique integer. We decided to increment this integer sequentially for simplicity.\n",
    "\n",
    "- Additionally, we created an `inverted_index` that maps these integers back to the specific documents in which the corresponding words appear. This setup allows us to define a `search_query` function where, by inputting a word or phrase, we can retrieve all documents containing all of those words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df= pd.read_csv(\"dataset.tsv\", sep='\\t', encoding= \"utf-8\")\n",
    "\n",
    "# Funzione per il preprocessamento e lo stemming del testo\n",
    "def preprocess_and_stem_text(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [re.sub(r\"[^a-zA-Z']\", '', word) for word in words]  # Rimuove caratteri non alfabetici\n",
    "    stop_words = set(stopwords.words('english'))  # Stopwords in inglese\n",
    "    stemmer = PorterStemmer()  # Stemmer di Porter\n",
    "    \n",
    "    filtered_stemmed_words = []\n",
    "    for word in words:\n",
    "        if word and word not in stop_words:\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            filtered_stemmed_words.append(stemmed_word)\n",
    "    \n",
    "    return filtered_stemmed_words\n",
    "\n",
    "df[\"processed_description\"] = df[\"description\"].apply(preprocess_and_stem_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Conjunctive Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Create Your Index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_vocabulary_and_inverted_index(df):\n",
    "\n",
    "    vocabulary = {}\n",
    "    inverted_index = defaultdict(list)\n",
    "    \n",
    "    term_id = 0\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        description = set(row[\"processed_description\"])\n",
    "        restaurant_id = row[\"index\"]\n",
    "\n",
    "        \n",
    "        for word in description:\n",
    "          \n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = term_id\n",
    "                term_id += 1\n",
    "\n",
    "\n",
    "            term_id_for_word = vocabulary[word]\n",
    "            inverted_index[term_id_for_word].append(restaurant_id)\n",
    "    \n",
    "    return vocabulary, inverted_index\n",
    "\n",
    "vocabulary, inverted_index = create_vocabulary_and_inverted_index(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save vocabulary as vocabulary.csv\n",
    "vocab_df = pd.DataFrame(list(vocabulary.items()), columns=[\"word\", \"term_id\"])\n",
    "vocab_df.to_csv(\"vocabulary.csv\", index=False)\n",
    "\n",
    "# Save inverted index as inverted_index.json\n",
    "with open(\"inverted_index.json\", \"w\") as f:\n",
    "    json.dump(inverted_index, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurantName</th>\n",
       "      <th>address</th>\n",
       "      <th>description</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20Tre</td>\n",
       "      <td>via David Chiossone 20 r</td>\n",
       "      <td>Situated in the heart of Genoa’s historic cent...</td>\n",
       "      <td>https://www.ristorante20tregenova.it/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Donevandro</td>\n",
       "      <td>via Garibaldi 2</td>\n",
       "      <td>Up until a few years ago, the owner-chef at th...</td>\n",
       "      <td>http://www.donevandroristorante.it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Etra</td>\n",
       "      <td>piazza De Ferrari 4</td>\n",
       "      <td>Etra is an anagram of the Italian word “arte” ...</td>\n",
       "      <td>https://www.etra.art/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Il Ristorante Alain Ducasse Napoli</td>\n",
       "      <td>Via Cristoforo Colombo 45</td>\n",
       "      <td>Alain Ducasse, one of the great names in conte...</td>\n",
       "      <td>https://theromeocollection.com/en/romeo-napoli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>La Buca</td>\n",
       "      <td>corso Garibaldi 45</td>\n",
       "      <td>Choose one of the tables on the outdoor summer...</td>\n",
       "      <td>https://www.labucaristorante.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>Erbaluigia</td>\n",
       "      <td>via San Frediano 10/12</td>\n",
       "      <td>This attractive restaurant with a simple, mini...</td>\n",
       "      <td>https://erbaluigia.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>Locanda 53 Supper Club</td>\n",
       "      <td>via Vergolano 53</td>\n",
       "      <td>Partners in life and business, Evelyn and Carl...</td>\n",
       "      <td>https://it.locanda53.it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>Sotto l'Arco</td>\n",
       "      <td>via Aretusi 5</td>\n",
       "      <td>Villa Aretusi is a pleasant 17C villa surround...</td>\n",
       "      <td>https://www.villa-aretusi.it/ristorante-sotto-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>Café Les Paillotes</td>\n",
       "      <td>piazza Le Laudi 2</td>\n",
       "      <td>This old acquaintance of the Michelin Guide no...</td>\n",
       "      <td>https://www.lespaillotes.it/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>Ristorante de LEN</td>\n",
       "      <td>Via Cesare Battisti 66</td>\n",
       "      <td>Just a stone’s throw from the central and very...</td>\n",
       "      <td>https://hoteldelen.it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>507 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          restaurantName                    address  \\\n",
       "0                                  20Tre   via David Chiossone 20 r   \n",
       "7                             Donevandro            via Garibaldi 2   \n",
       "8                                   Etra        piazza De Ferrari 4   \n",
       "9     Il Ristorante Alain Ducasse Napoli  Via Cristoforo Colombo 45   \n",
       "11                               La Buca         corso Garibaldi 45   \n",
       "...                                  ...                        ...   \n",
       "1964                          Erbaluigia     via San Frediano 10/12   \n",
       "1968              Locanda 53 Supper Club           via Vergolano 53   \n",
       "1976                        Sotto l'Arco              via Aretusi 5   \n",
       "1981                  Café Les Paillotes          piazza Le Laudi 2   \n",
       "1982                   Ristorante de LEN     Via Cesare Battisti 66   \n",
       "\n",
       "                                            description  \\\n",
       "0     Situated in the heart of Genoa’s historic cent...   \n",
       "7     Up until a few years ago, the owner-chef at th...   \n",
       "8     Etra is an anagram of the Italian word “arte” ...   \n",
       "9     Alain Ducasse, one of the great names in conte...   \n",
       "11    Choose one of the tables on the outdoor summer...   \n",
       "...                                                 ...   \n",
       "1964  This attractive restaurant with a simple, mini...   \n",
       "1968  Partners in life and business, Evelyn and Carl...   \n",
       "1976  Villa Aretusi is a pleasant 17C villa surround...   \n",
       "1981  This old acquaintance of the Michelin Guide no...   \n",
       "1982  Just a stone’s throw from the central and very...   \n",
       "\n",
       "                                                website  \n",
       "0                 https://www.ristorante20tregenova.it/  \n",
       "7                    http://www.donevandroristorante.it  \n",
       "8                                 https://www.etra.art/  \n",
       "9     https://theromeocollection.com/en/romeo-napoli...  \n",
       "11                    https://www.labucaristorante.com/  \n",
       "...                                                 ...  \n",
       "1964                            https://erbaluigia.com/  \n",
       "1968                            https://it.locanda53.it  \n",
       "1976  https://www.villa-aretusi.it/ristorante-sotto-...  \n",
       "1981                       https://www.lespaillotes.it/  \n",
       "1982                              https://hoteldelen.it  \n",
       "\n",
       "[507 rows x 4 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def execute_query(query, df):\n",
    "    # Preprocessa i termini della query utilizzando la funzione preprocess_and_stem_text\n",
    "    processed_query = preprocess_and_stem_text(query)\n",
    "    query_terms = set(processed_query)\n",
    "    \n",
    "    # Trova gli indici dei ristoranti che soddisfano la query\n",
    "    matching_indices = df[df[\"processed_description\"].apply(lambda desc: query_terms.issubset(desc))].index\n",
    "    \n",
    "    # Usa iloc con gli indici trovati per selezionare le righe e filtrare solo le colonne desiderate\n",
    "    result = df.iloc[matching_indices][[\"restaurantName\", \"address\", \"description\", \"website\"]]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Prompt per inserire la query\n",
    "query = input(\"Please enter your search terms (e.g., 'modern seasonal cuisine'): \")\n",
    "\n",
    "# Esegui la query e ottieni i risultati\n",
    "result = execute_query(query, df)\n",
    "\n",
    "# Visualizza i risultati\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Ranked Search Engine with TF-IDF and Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Inverted Index with TF-IDF Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#TF-IDF scores\n",
    "def build_new_inverted_index(data):\n",
    "    # Vectorize descriptions using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(data['description'])\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # inverted index with TF-IDF scores\n",
    "    inverted_index = defaultdict(list)\n",
    "    for doc_id, row in enumerate(tfidf_matrix):\n",
    "        for term_id, tfidf_score in zip(row.indices, row.data):\n",
    "            term = terms[term_id]\n",
    "            inverted_index[term].append((doc_id, tfidf_score))\n",
    "\n",
    "    return inverted_index, vectorizer\n",
    "\n",
    "# Generate the inverted index\n",
    "inverted_index, vectorizer = build_new_inverted_index(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Execute the Ranked Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurantName</th>\n",
       "      <th>address</th>\n",
       "      <th>description</th>\n",
       "      <th>website</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La Botte</td>\n",
       "      <td>via Giuseppe Garibaldi 8</td>\n",
       "      <td>A modern and welcoming contemporary bistro sit...</td>\n",
       "      <td>http://www.trattorialabottestresa.it</td>\n",
       "      <td>0.388885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Osteria del Borgo</td>\n",
       "      <td>via Pietro Custodi 5</td>\n",
       "      <td>Situated in the heart of the old town, this re...</td>\n",
       "      <td>tel:+39 349 160 3750</td>\n",
       "      <td>0.378145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Braunwirt</td>\n",
       "      <td>piazza Chiesa 3</td>\n",
       "      <td>A modern and welcoming restaurant in the heart...</td>\n",
       "      <td>https://www.braunwirt.it/</td>\n",
       "      <td>0.330758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Degusteria Italiana</td>\n",
       "      <td>via Lambertesca 7r</td>\n",
       "      <td>This small, welcoming restaurant in the heart ...</td>\n",
       "      <td>http://www.degusteriaitalianafirenze.com</td>\n",
       "      <td>0.284699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        restaurantName                   address  \\\n",
       "0             La Botte  via Giuseppe Garibaldi 8   \n",
       "1    Osteria del Borgo      via Pietro Custodi 5   \n",
       "2            Braunwirt           piazza Chiesa 3   \n",
       "3  Degusteria Italiana        via Lambertesca 7r   \n",
       "\n",
       "                                         description  \\\n",
       "0  A modern and welcoming contemporary bistro sit...   \n",
       "1  Situated in the heart of the old town, this re...   \n",
       "2  A modern and welcoming restaurant in the heart...   \n",
       "3  This small, welcoming restaurant in the heart ...   \n",
       "\n",
       "                                    website  similarity_score  \n",
       "0      http://www.trattorialabottestresa.it          0.388885  \n",
       "1                      tel:+39 349 160 3750          0.378145  \n",
       "2                 https://www.braunwirt.it/          0.330758  \n",
       "3  http://www.degusteriaitalianafirenze.com          0.284699  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# TF-IDF vectors for each restaurant description\n",
    "def preprocess_data(data):\n",
    "    vectorizer = TfidfVectorizer()  # Use custom stop words\n",
    "    tfidf_matrix = vectorizer.fit_transform(data['description'])\n",
    "    return tfidf_matrix, vectorizer\n",
    "\n",
    "# Execute the ranked query using TF-IDF and Cosine Similarity\n",
    "def execute_ranked_query(query, data, tfidf_matrix, vectorizer, k):\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    \n",
    "    # Calculate cosine similarity between the query and all documents\n",
    "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    \n",
    "    # Get indices of documents sorted by score\n",
    "    ranked_indices = np.argsort(similarity_scores)[::-1][:k]  # Top k\n",
    "    \n",
    "    results = data.iloc[ranked_indices][['restaurantName', 'address', 'description', 'website']].copy()\n",
    "    results['similarity_score'] = similarity_scores[ranked_indices]\n",
    "    results.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Build TF-IDF matrix\n",
    "tfidf_matrix, vectorizer = preprocess_data(df)\n",
    "\n",
    "# Prompt user for input and display results\n",
    "query = input(\"Please enter your search terms (e.g., 'modern seasonal cuisine'): \")\n",
    "k = int(input(\"How many top similar restaurants would you like to see? \"))\n",
    "result = execute_ranked_query(query, df, tfidf_matrix, vectorizer, k)\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
